{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4aa922",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn (sklearn)\n",
    "\n",
    "This notebook demonstrates some of the most useful functions of the beautiful Scikit-Learn library.\n",
    "\n",
    "what we're going to cover:\n",
    "\n",
    "0. An end-to-end Scikit-Learn workflow\n",
    "1. Getting the data ready\n",
    "2. Choose yhe right estimator/algorithm for our problems\n",
    "3. Fit the model/algorithm and use it to make predictions on our data\n",
    "4. Evaluate a model \n",
    "5. Improve a model\n",
    "6. Save and load a trained model\n",
    "7. Putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72239979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730fef20",
   "metadata": {},
   "source": [
    "## 0. An End-to-End Scikit-Learn workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77bfcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get the Data ready\n",
    "import pandas as pd\n",
    "heart_disease = pd.read_csv(\"data/heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a1beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X (features matrix)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# create y (label matrix)\n",
    "y = heart_disease[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d450b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd760dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Choose the right model and hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# We'll keep the default hyperparameters\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830c762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit the model to the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0c294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]\n",
      "executable: C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\python.exe\n",
      "   machine: Windows-10-10.0.22000-SP0\n",
      "\n",
      "Python dependencies:\n",
      "      sklearn: 1.1.1\n",
      "          pip: 22.0.4\n",
      "   setuptools: 63.4.1\n",
      "        numpy: 1.23.1\n",
      "        scipy: 1.7.3\n",
      "       Cython: None\n",
      "       pandas: 1.4.3\n",
      "   matplotlib: 3.5.1\n",
      "       joblib: 1.1.0\n",
      "threadpoolctl: 2.2.0\n",
      "\n",
      "Built with OpenMP: True\n",
      "\n",
      "threadpoolctl info:\n",
      "       filepath: C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\Library\\bin\\mkl_rt.1.dll\n",
      "         prefix: mkl_rt\n",
      "       user_api: blas\n",
      "   internal_api: mkl\n",
      "        version: 2021.4-Product\n",
      "    num_threads: 4\n",
      "threading_layer: intel\n",
      "\n",
      "       filepath: C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\vcomp140.dll\n",
      "         prefix: vcomp\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "        version: None\n",
      "    num_threads: 8\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6cb6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87387fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_label \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:832\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    872\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    873\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 874\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    877\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:605\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 605\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "y_label = clf.predict(np.array([0,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34475ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model on the training data and test data\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312302d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0193e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improve a Model\n",
    "# Try different amounts of n_estimators\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(10, 100, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    print(f\"Model accuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save a model and load it \n",
    "import pickle \n",
    "\n",
    "pickle.dump(clf, open(\"random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"random_forest_model_1.pkl\", \"rb\"))\n",
    "loaded_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe880ae",
   "metadata": {},
   "source": [
    "## 1. Getting our data ready to be used with machine learning\n",
    "\n",
    "\n",
    "- Three main things we have to do:\n",
    "    1. Split type data into features and labels (unusually 'X' and 'y').\n",
    "    2. Filling (also called imputing) or disregarding missing values.\n",
    "    3. Converting non-numerical values to numerical values also called (also called feature encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61eb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95107fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart_disease[\"target\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76830129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f169a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33801c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e36988",
   "metadata": {},
   "source": [
    "### 1.1 Make sure it's all numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = pd.read_csv(\"data/car-sales-extended.csv\")\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa120b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X/y\n",
    "X = car_sales.drop(\"Price\", axis=1)\n",
    "y = car_sales[\"Price\"]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f205e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn categories into numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\", \"Doors\"]])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refit the model\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e4a5b",
   "metadata": {},
   "source": [
    "### 1.2 What if there were missing values?\n",
    "\n",
    "1. Fill them with some value (also known as imputation)\n",
    "2. Remove the samples with missing data altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import car sales missing data \n",
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e937775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y \n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e761dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try and convert our data to numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing[\"Doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf64bce",
   "metadata": {},
   "source": [
    "### Option 1: Fill missing data with Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fa536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the \"Make\" column  \n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Colour\" column \n",
    "car_sales_missing[\"Colour\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Odometer (KM)\" column \n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True)\n",
    "\n",
    "# Fill the \"Doors\" column \n",
    "car_sales_missing[\"Doors\"].fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our DataFrame again\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bafdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing price values \n",
    "car_sales_missing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a94c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55093eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try and convert our data to numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(car_sales_missing)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845e316",
   "metadata": {},
   "source": [
    "### Option 2: Fill missing values with Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8061652",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the labels with no labels \n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with Scikit-Learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Fill categorical values with 'missing' & numerical values with mean\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Define columns \n",
    "cat_features = [\"Make\", \"Colour\"]\n",
    "door_features = [\"Doors\"]\n",
    "num_features = [\"Odometer (KM)\"]\n",
    "\n",
    "# Create an imputer (something that fill the missing data)\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),\n",
    "    (\"door_imputer\", door_imputer, door_features),\n",
    "    (\"num_imputer\", num_imputer, num_features)\n",
    "])\n",
    "\n",
    "# Transform the data \n",
    "filled_X = imputer.fit_transform(X)\n",
    "filled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494db8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled = pd.DataFrame(filled_X,\n",
    "                                columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "car_sales_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ad70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try and convert our data to numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(car_sales_filled)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got our data as numbers and filled (no missing values)\n",
    "# Let's fit a model\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20332c13",
   "metadata": {},
   "source": [
    "## 2. Choosing the right estimator/algorithm for your problem\n",
    "\n",
    "Some things to note:\n",
    "    \n",
    "* Sklearn refers to machine learning models, algorithms as estimators\n",
    "* Classification problem - predicting a category (heart disease or not)\n",
    "    * Sometimes you'll see `clf` (short for classifier) used as classification estimator \n",
    "* Regression problem - predicting a number (selling price of a car)\n",
    "\n",
    "If you're working on a machine ;earning problem and looking to use Sklearn and not sure what model you should use, refer to the sklearn machine learning map: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8d9a5",
   "metadata": {},
   "source": [
    "### 2.1 Picking a machine learning model for a regression model\n",
    "\n",
    "Let's use the California Housing dataset - https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75181a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"target\"] = housing[\"target\"]\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31532867",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithm/estimator\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setup Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # median house price in $100,000s\n",
    "\n",
    "# Split into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model (on the training set)\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on a test set)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550748e5",
   "metadata": {},
   "source": [
    "What if `Ridge` didn't work or the score didn't fit our needs?\n",
    "\n",
    "Well, we could always try a different model...\n",
    "\n",
    "How about we try an ensemble model (an ensemble is combination of smaller models to try and make better predictions than just a single model)?\n",
    "\n",
    "Sklearn's ensemble_models can be found here: https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed640a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data \n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create random forest model\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36f086",
   "metadata": {},
   "source": [
    "## 2.2 Picking a machine learning model for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\"data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cd3e9",
   "metadata": {},
   "source": [
    "Consulting the map and it says to try `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearSVC estimator class\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate LinearSVC \n",
    "clf = LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LinearSVC\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd332c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier \n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest Classifier \n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1332e",
   "metadata": {},
   "source": [
    "Tidbit: \n",
    "\n",
    "    1. If you have structured data, used ensemble methods \n",
    "    2. If you have unstructured data, use deep learning or transfer learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f572fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb0310",
   "metadata": {},
   "source": [
    "## 3. Fit the model/algorithm on our data and use it to make predictions\n",
    "\n",
    "### 3.1 Fitting the model to the data \n",
    "\n",
    "Different names for:\n",
    "* `X` = features, feature variables, data\n",
    "* `y` = labels, targets, target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d98a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier \n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data (training the machine learning model)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest Classifier (use the patterns that model has learned)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ad031",
   "metadata": {},
   "source": [
    "### 3.2 Make predictions using a machine learning model \n",
    "\n",
    "2 ways to make a predictions:\n",
    "1. `predict()`\n",
    "2. `predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to make predictions \n",
    "clf.predict(np.array([1, 7, 8, 3, 4]))  # this doesn't work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to truth labels to evaluate the model\n",
    "y_preds = clf.predict(X_test)\n",
    "np.mean(y_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b632279",
   "metadata": {},
   "source": [
    "Make predictions with `predic_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba() returns probabilities of aclassification label\n",
    "clf.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict() on the same data.....\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3967857",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166e89f",
   "metadata": {},
   "source": [
    "`predict()` can also be used for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create Model instance\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions \n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5781d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predictions to the truth\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380ecf4",
   "metadata": {},
   "source": [
    "## 4. Evaluating the machine learning model \n",
    "\n",
    "Three ways to evaluate Scikit-Learn models/estimators:\n",
    "\n",
    "1. Estimator's built-in `score()` method\n",
    "2. The `scoring` parameter\n",
    "3. Problem-specific metric functions\n",
    "    \n",
    "Read more about these here: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c03be2",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating model with `score` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58225bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier \n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data (training the machine learning model)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest value for the .score() is the 1.0, the lowest 0.0\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a90d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f7d21",
   "metadata": {},
   "source": [
    "Let's use the `score()` on our regression problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create Random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create Model instance\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c350410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default score() evaluation metric is r_squared for regression algorithms \n",
    "# Highest: 1.0, Lowest: 0.0\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d881d",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating a model using the `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier \n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data (training the machine learning model)\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45623652",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "# Single training and test split score \n",
    "clf_single_score = clf.score(X_test, y_test)\n",
    "\n",
    "# Take the  mean of 5-fold cross-validation score \n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))\n",
    "\n",
    "# Compare the two \n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scoring parameter of classifier = mean accuracy\n",
    "clf.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring parameter set to None by default \n",
    "\n",
    "cross_val_score(clf, X, y, cv=5, scoring=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584db98",
   "metadata": {},
   "source": [
    "### 4.2.1 Classification model evaluation metrics \n",
    "\n",
    "1. Accuracy \n",
    "2. Area under ROC Curve\n",
    "3. Confusion metrics\n",
    "4. Classification report \n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7703e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "cross_val_score = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93116b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c12b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier Cross-Validated Accuracy: {np.mean(cross_val_score) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e2108",
   "metadata": {},
   "source": [
    "**Area under the reciever operating characteristics curve(AUC/ROC)**\n",
    "\n",
    "* Area under curve (AUC)\n",
    "* ROC curve\n",
    "\n",
    "ROC curves are a comparison of a model's true positive rate (tpr) versus a models false positive rate (fpr)\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_train, X_test, y_train, y_test ... etc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Fit the classifier \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with probabilities \n",
    "y_probs = clf.predict_proba(X_test, )\n",
    "\n",
    "y_probs[:10], len(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec61f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive = y_probs[:, 1]\n",
    "y_probs_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71106b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr and thresholds \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "# Check the false positive rates \n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9086e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for plotting ROC curves \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots are ROC Curve given the false positive rate (fpr) and true positive rate (tpr) \n",
    "    of a model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "    \n",
    "    # Plot line with no predictive power (baseline)\n",
    "    plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Guessing\")\n",
    "    \n",
    "    # Customize the plot \n",
    "    plt.xlabel(\"False Positive Rate (fpr)\")\n",
    "    plt.ylabel(\"True Positive Rate (tpr)\")\n",
    "    plt.title(\"Reciever Operating Charactertic (ROC) Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87359c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_probs_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perfect ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35346b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect AUC score\n",
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0674995",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts an the actual labels it was supposed to predict.\n",
    "\n",
    "In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da892d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix using pd.crosstab()\n",
    "pd.crosstab(y_test,\n",
    "            y_preds, \n",
    "            rownames=[\"Actual Labels\"],\n",
    "            colnames=[\"Predicted Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa77814",
   "metadata": {},
   "outputs": [],
   "source": [
    "22 + 3 + 5 + 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26cffa",
   "metadata": {},
   "source": [
    "**Installing Seaborn right in jupyter notebook**\n",
    "\n",
    "`\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} seaborn\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our confusion matrix more visual with Seaborn's heatmap()\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the font scale \n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "# Plot it using Seaborn \n",
    "sns.heatmap(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97a5ba",
   "metadata": {},
   "source": [
    "**Confusion matrix (Refresher)**\n",
    "\n",
    "The next way to evaluate a classification model is by using a confusion matrix\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict\n",
    "\n",
    "In essence, this gives you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn confusion matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,\n",
    "            y_preds,\n",
    "            rownames=[\"Actual Labels\"],\n",
    "            colnames=[\"Predicted Labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4563a",
   "metadata": {},
   "source": [
    "### Creating a confusion matrix using Scikit-Learn \n",
    "\n",
    "To use the new methods of creating a confusion matrix with Scikit-Learn 1.0+ : https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=clf,\n",
    "                                      X=X,\n",
    "                                      y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c93aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test,\n",
    "                                        y_pred=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffa941",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where precision and recall become valuable \n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one positive case\n",
    "\n",
    "disease_preds = np.zeros(10000) # model predicts every case as 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,\n",
    "                                   disease_preds,\n",
    "                                   output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6433cef",
   "metadata": {},
   "source": [
    "### 4.2.2 Regresison model evaluation metrics \n",
    "\n",
    "Model evaluation metrics documentation: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "The ones we're going to cover are:\n",
    "\n",
    "1. R^2 (Pronounced r-squared) or coefficient of determination \n",
    "2. Mean absolute error (MAE)\n",
    "3. Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3009e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d191bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed357669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Fill an array with y_test mean\n",
    "y_test_mean = np.full(len(y_test), y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12021723",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_test,\n",
    "         y_pred=y_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c242b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_test,\n",
    "         y_pred=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c74fc",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE)**\n",
    " \n",
    "MAE is the average of the absolute differences between predictions and actual values.\n",
    " \n",
    "It gives you an idea of how wrong your models predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b08d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\": y_test,\n",
    "                        \"predicted values\": y_preds})\n",
    "df[\"differences\"] = df[\"predicted values\"] - df[\"actual values\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE using formulas and differences \n",
    "np.abs(df[\"differences\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b47a7",
   "metadata": {},
   "source": [
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "MSE is the mean of the square of the errors between actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a80c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"squared_differences\"] = np.square(df[\"differences\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE by hand \n",
    "squared = np.square(df[\"differences\"])\n",
    "squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80755832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_larged_error = df.copy()\n",
    "df_larged_error.iloc[0][\"squared_differences\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_larged_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99214075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE with large error \n",
    "df_larged_error[\"squared_differences\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_larged_error.iloc[1:100] = 20\n",
    "df_larged_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE with large error \n",
    "df_larged_error[\"squared_differences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acb2c3",
   "metadata": {},
   "source": [
    "### 4.2.3 Finally using the `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760dbeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f67350",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Cross-validation accuracy \n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=None) # if scoring=None, estimator's default scoring evaluation metric is used (accuracy for classification models)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fe366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a26741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "cv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d257c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated precision\n",
    "print(f\"The cross-validated precision is: {np.mean(cv_precision)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "np.random.seed(42)\n",
    "cv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90551d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated recall\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e2c4e",
   "metadata": {},
   "source": [
    "Let's see `scoring` parameter being using for a regression problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ee23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X, y, cv=3, scoring=None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8544d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error \n",
    "cv_mae = cross_val_score(model, X, y, cv=3, scoring=\"neg_mean_absolute_error\")\n",
    "np.mean(cv_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50512787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "cv_mse = cross_val_score(model, X, y, cv=3, scoring=\"neg_mean_squared_error\")\n",
    "np.mean(cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfd914",
   "metadata": {},
   "source": [
    "### 4.3 Using different metrics as Scikit-Learn Functions \n",
    "\n",
    "The 3rd way to evaluate scikit-learn machine learning models/estimators is to using the `sklearn.metrics`\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4906e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier metrics on the test set ðŸ‘‡\n",
      "Accuracy: 85.25%\n",
      "Precision: 0.8484848484848485\n",
      "Recall: 0.875\n",
      "F1: 0.8615384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X and Y\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create Model\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit Model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions \n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Evaluate Model using evaluation functions\n",
    "print(\"Classifier metrics on the test set ðŸ‘‡\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_preds) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_preds)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_preds)}\")\n",
    "print(f\"F1: {f1_score(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff86c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X and Y\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create Model\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# Evaluate Model using evaluation functions\n",
    "print(\"Regression metrics on the test set ðŸ‘‡\")\n",
    "print(f\"R2 score: {r2_score(y_test, y_preds)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_preds)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a36b7e",
   "metadata": {},
   "source": [
    "## 5. Improving a model\n",
    "\n",
    "First predictions = baseline predictions.\n",
    "First model = baseline model.\n",
    "\n",
    "From a data perspective:\n",
    "* Could we collect more data? (generally, the more data, the better)\n",
    "* Could we improve our data?\n",
    "\n",
    "From a model perspective:\n",
    "* Is there a better model we could use?\n",
    "* Could we improve a current model?\n",
    "\n",
    "Hyperparameters vs. Parameters \n",
    "* parameters = model find those patterns in data\n",
    "* hyperparameters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters:\n",
    "1. By Hand \n",
    "2. Randomly with RandomSearchCV\n",
    "3. Exhaustvely with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d98124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3981da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26becc",
   "metadata": {},
   "source": [
    "### 5.1 Tuning hyperparameters by hand \n",
    "\n",
    "Let's make 3 sets, training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432ca6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672f501",
   "metadata": {},
   "source": [
    "We're going to try and adjust:\n",
    "\n",
    "* `max_depth` \n",
    "* `max_features`\n",
    "* `min_samples_leaf`\n",
    "* `n_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b20ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true ,y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_preds labels on a \n",
    "    classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    \n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11751424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 80.00%\n",
      "Precision: 0.77\n",
      "Recall: 0.92\n",
      "F1 Score: 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'precision': 0.77, 'recall': 0.92, 'f1': 0.84}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "heart_disease_shuffled = heart_disease.sample(frac=1)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split the data into train, validation & test sets \n",
    "train_split = round(0.7 * len(heart_disease_shuffled)) # 70% of data \n",
    "valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of data \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]\n",
    "X_test, y_test = X[valid_split:], y[valid_split:]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make baseline predictions \n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier on validation set \n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57a74fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 82.22%\n",
      "Precision: 0.84\n",
      "Recall: 0.84\n",
      "F1 Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a second classifier with different hyperparameters\n",
    "clf_2 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with different hyperparameters \n",
    "y_preds_2 = clf_2.predict(X_valid)\n",
    "\n",
    "# Evaluate the 2nd classifier \n",
    "clf_2_metrics = evaluate_preds(y_valid, y_preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccd765",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b36e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   1.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   1.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   1.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   1.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=10; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1000; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1000; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1000; total time=   0.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1000; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1000; total time=   0.9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid  = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "         \"max_depth\": [None, 5, 10, 20, 30],\n",
    "         \"max_features\": [\"auto\", \"sqrt\"],\n",
    "         \"min_samples_split\": [2, 4, 6],\n",
    "         \"min_samples_leaf\": [1, 2, 4]},\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=grid,\n",
    "                            n_iter=10, #number of models to try\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the RandomizedSaerchCV version of clf \n",
    "rs_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3d2aa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "045b9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 81.97%\n",
      "Precision: 0.77\n",
      "Recall: 0.86\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds = rs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics = evaluate_preds(y_test, rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925bbfd",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e368473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': [10, 100, 200, 500, 1000, 1200],\n",
       "  'max_depth': [None, 5, 10, 20, 30],\n",
       "  'max_features': ['auto', 'sqrt'],\n",
       "  'min_samples_split': [2, 4, 6],\n",
       "  'min_samples_leaf': [1, 2, 4]},)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "774f7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {'n_estimators': [100, 200, 500],\n",
    "          'max_depth': [None],\n",
    "          'max_features': ['auto', 'sqrt'],\n",
    "          'min_samples_split': [6],\n",
    "          'min_samples_leaf': [1, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93c12cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500; total time=   0.4s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator=clf,\n",
    "                      param_grid=grid_2,\n",
    "                      cv=5,\n",
    "                      verbose=2)\n",
    "\n",
    "# Fit the GridSaerchCV version of clf \n",
    "gs_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e38ae2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0bf874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 78.69%\n",
      "Precision: 0.74\n",
      "Recall: 0.82\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions \n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844717e1",
   "metadata": {},
   "source": [
    " Let's compare our different model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b341d56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,\n",
    "                                \"clf_2\": clf_2_metrics,\n",
    "                                \"random search\": rs_metrics,\n",
    "                                \"grid search\": gs_metrics})\n",
    "\n",
    "compare_metrics.plot.bar(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2e370",
   "metadata": {},
   "source": [
    "## 6. Saving and oading the trained machine learning models\n",
    "\n",
    "Two ways to save and load machine learning models:\n",
    "1. With Pythons `pickle` module\n",
    "2. With the `joblib` module \n",
    "\n",
    "**Pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c031a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an existing model to file \n",
    "pickle.dump(gs_clf, open(\"gs_random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0379ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model \n",
    "loaded_pickle_model = pickle.load(open(\"gs_random_forest_model_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b4ebfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 78.69%\n",
      "Precision: 0.74\n",
      "Recall: 0.82\n",
      "F1 Score: 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'precision': 0.74, 'recall': 0.82, 'f1': 0.78}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions \n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test, pickle_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402387a9",
   "metadata": {},
   "source": [
    "**Joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "247eaf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save model to file \n",
    "dump(gs_clf, filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adedcf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a saved joblib model\n",
    "loaded_job_model = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c8e8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 78.69%\n",
      "Precision: 0.74\n",
      "Recall: 0.82\n",
      "F1 Score: 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'precision': 0.74, 'recall': 0.82, 'f1': 0.78}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and evaluate joblib predictions \n",
    "joblib_y_preds = loaded_job_model.predict(X_test)\n",
    "evaluate_preds(y_test, joblib_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58c2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
